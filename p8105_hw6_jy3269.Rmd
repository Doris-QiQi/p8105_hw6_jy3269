---
title: "p8105_hw6_jy3269"
author: "Jingyi Yao"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


## Problem 1

To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 


\ \par
\ \par
## Problem 2

### 1. Create `city_state` variable and a binary `solved` variable 
```{r}
raw_data <- read_csv("./data/homicide-data.csv",show_col_types = FALSE)
homicide <- raw_data %>% 
  mutate(
    city_state = str_c(city,", ",state),
    solved = ifelse(disposition == "Closed by arrest",1,0)) %>% 
  mutate(
      victim_sex = fct_relevel(victim_sex, "Female"),
      victim_race = fct_relevel(victim_race, "White"),
         ) 

homicide

```

## 2. Omit cities : Dallas, TX; Phoenix, AZ; and Kansas City, MO  and  Tulsa, AL
```{r}
homicide <- homicide %>% 
  filter(city_state != "Dallas, TX", city_state != "Phoenix, AZ", city_state != "Kansas City, MO", city_state != "Tulsa, AL")

```


## 3. Limit your analysis those for whom victim_race is white or black
```{r}
homicide <- homicide %>% 
  filter(victim_race == "White" | victim_race == "Black") %>% 
  mutate(victim_age = as.numeric(victim_age))

```

## 4. Select the predictors and response variable in Baltimore
```{r}
baltimore_df = 
  homicide %>% 
  filter(city == "Baltimore") %>% 
  select(solved, victim_age, victim_race, victim_sex)

baltimore_df

```


## 5. Fit logistic regression for Baltimore
```{r}
Baltimore_logistic = 
  baltimore_df %>% 
  glm(solved ~ victim_age + victim_race + victim_sex, data = ., family = binomial())

```



## 6. Save the Baltimore model result as an R Object
```{r}
save(Baltimore_logistic, file = "./result/Baltimore_logistic.RData")

```



## 7. Tidy the object
```{r}
Baltimore_logistic %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)


```


## 8. Get the 95% CI of OR 
```{r}
alpha = 0.05

Baltimore_logistic %>% 
  broom::tidy() %>%
  mutate(OR = exp(estimate),
         OR_lower = exp(estimate - 1.96*std.error),
         OR_upper = exp(estimate + 1.96*std.error) ) %>%
  filter(term == 'victim_sexMale') %>% 
  select(estimate, OR,OR_lower,OR_upper) %>% 
  knitr::kable(digits = 3)


```


The OR result shows that male victims in the homicides have a lower chance to have the case solved compared with women.


## 9.  Define a `glm_all()` function
```{r,message=FALSE, warning=FALSE}
glm_all = function(citystate){
  city_glm = homicide %>% 
    filter(city_state == citystate) %>% 
    glm(solved ~ victim_sex + victim_race + victim_age, family = binomial, data = .) %>% 
    broom::tidy() %>% 
    mutate(
         OR = exp(estimate), 
         OR_lower = exp(estimate - 1.96*std.error), 
         OR_upper = exp(estimate + 1.96*std.error)) %>% 
    filter(term == "victim_sexMale") %>% 
    select(OR, OR_lower, OR_upper)
    
    city_glm
}
```


## 10. apply `glm_all()` to all the cities and tidy the result
```{r}
city_state_list = homicide %>% 
  select(city_state) %>% 
  unique()

glm_all_result = city_state_list %>% 
  mutate(glm_result = map(city_state, glm_all)) %>% 
  unnest(glm_result) %>% 
  arrange(desc(OR)) 

glm_all_result %>%  knitr::kable(digits = 3)
```



```{r}
ggplot(glm_all_result, aes(y = fct_reorder(city_state, OR), x = OR)) +
  geom_point(color = "red", lwd = 2) +
  geom_errorbar(aes(xmin = OR_lower, xmax = OR_upper)) +
  labs(title = "Estimated OR and 95% CI") + ylab("City,State") + theme(axis.text.y = element_text(hjust = 0.5,size = 8), axis.text.x = element_text(size = 16), axis.title.x = element_text(size = 20), axis.title.y = element_text(size = 20),title = element_text(size = 20))

```


\ \par
\ \par
Comments on the plot :

  * Most of the estimated OR is less than 1. Thus, in most cities, the homicide whose victim is a male is less likely to be solved compared to those with a female victim.
  * Albuquerque, Stockton, Fresno, Nashville and Richmond have an estimated OR that is above 1. However, their 95% CI contains 1. Thus, we cannot conclude that there is significant difference between the odds of solving homicides for male and female victims in these cities.
  * Some cities' 95% CI does not contain 1, which means that there are significant difference between the odds of solving homicides for male and female victims in these cities. For example, New York, Chicago are cities with CI that does not contain 1.



\ \par
\ \par
## Problem 3

```{r,message=FALSE}
data <- read_csv("./data/birthweight.csv")
```

### 1. Clean the data for regression analysis

```{r}
birthwt <- data %>% 
  mutate(babysex = as.factor(babysex),
         frace = as.factor(frace),
         malform = as.factor(malform),
         mrace = as.factor(mrace)) %>%
  janitor::clean_names() %>% 
  select(bwt,everything())

# show the first 6 rows of the cleaned dataset
head(birthwt)


# check the missing values
sum(is.na(birthwt))
```


  Thus, the data has no missing values.


\ \par
\ \par
Next, I will fit my linear model using the cleaned data. I examined the meaning of each variable and find it reasonable that all the variables may be relevant to the birth weight of a baby. Thus, I plan to fit the model using all the variables other than `bwt` (the response variable) as predictors to build a full model first. Then I will use the step-wise method to select significant variables backwards. The selected predictors and the response variable will build up my regression model.



## 2. Fit a full model 
```{r}
full_model <- lm(bwt ~., data = birthwt)

broom::tidy(full_model) %>%  knitr::kable()
  
```



## 3. Select variables in the full model 
```{r}
stepwise_model <- MASS::stepAIC(full_model, direction = "backward", trace = FALSE)

broom::tidy(stepwise_model) %>%  knitr::kable()
```


\ \par
\ \par
As is shown in the stepwise_model result, the selected predictors are : babysex, bhead, blength, delwt, fincome, gaweeks, mheight, mrace, parity, ppwt, smoken. 


\ \par
The residual against fitted value plot is shown below.


## 4. Plot residuals vs. fitted values
```{r}
birthwt %>%  
  add_predictions(stepwise_model) %>% 
  add_residuals(stepwise_model) %>% 
  ggplot(aes(x = pred, y = resid)) + geom_point() +
  labs(title = "Residuals vs Fitted Values ", ) + xlab("Fitted Value") + ylab("Residuals")

```


## 5. Split dataset 
```{r}
cv_df = 
  crossv_mc(birthwt, 100) %>% 
  mutate(
    train = map(train, as_tibble), 
    test = map(test, as_tibble))

```



## 6. Compare the 3 models -- mean rmse
```{r,warning=FALSE}
cv_df %>% 
  mutate(
    my_model  = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
    model_2  = map(train, ~lm(bwt ~ gaweeks + blength, data = .x)),
    model_3  = map(train, ~lm(bwt ~ bhead*blength*babysex, data = .x))) %>% 
  mutate(
    rmse_my_model = map2_dbl(my_model, test, ~rmse(model = .x, data = .y)),
    rmse_2    = map2_dbl(model_2, test, ~rmse(model = .x, data = .y)),
    rmse_3 = map2_dbl(model_3, test, ~rmse(model = .x, data = .y))) %>% 
  summarize(
    avg_rmse_my_model = mean(rmse_my_model),
    avg_rmse_model_2 = mean(rmse_2),
    avg_rmse_model_3  = mean(rmse_3)
  ) %>% 
  knitr::kable()



```



```{r,warning=FALSE}
cv_df %>% 
  mutate(
    my_model  = map(train, ~lm( bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
    model_2  = map(train, ~lm(bwt ~ gaweeks + blength, data = .x)),
    model_3  = map(train, ~lm(bwt ~ bhead*blength*babysex, data = .x))) %>% 
  mutate(
    rmse_my_model = map2_dbl(my_model, test, ~rmse(model = .x, data = .y)),
    rmse_2    = map2_dbl(model_2, test, ~rmse(model = .x, data = .y)),
    rmse_3 = map2_dbl(model_3, test, ~rmse(model = .x, data = .y))) %>%
  select(starts_with("rmse")) %>%  
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin(aes(fill = model))
  

```


\ \par
\ \par
According to the table and violin plot above, we know that my model has the lowest mean RMSE. The main effect model (model 2) has the highest RMSE. The interaction model (model 3) has RMSE slightly higher than my model.








